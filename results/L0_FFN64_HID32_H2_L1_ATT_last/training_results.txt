==== Training Results ====

[T | 1h] - AR(1) model
- Train size: 7466
- Trainable params: 2
- Training time (s): 0.0029938220977783203
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 7466
Model:                     AutoReg(1)   Log Likelihood                4340.528
Method:               Conditional MLE   S.D. of innovations              0.135
Date:                Sun, 31 Aug 2025   AIC                          -8675.056
Time:                        14:45:20   BIC                          -8654.302
Sample:                             1   HQIC                         -8667.927
                                 7466                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const      -4.057e-06      0.002     -0.003      0.998      -0.003       0.003
T.L1           0.9899      0.002    601.255      0.000       0.987       0.993
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.0102           +0.0000j            1.0102            0.0000
-----------------------------------------------------------------------------


[T | 6h] - AR(1) model
- Train size: 1244
- Trainable params: 2
- Training time (s): 0.0020329952239990234
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 1244
Model:                     AutoReg(1)   Log Likelihood               -1753.245
Method:               Conditional MLE   S.D. of innovations              0.992
Date:                Sun, 31 Aug 2025   AIC                           3512.489
Time:                        14:45:32   BIC                           3527.865
Sample:                             1   HQIC                          3518.271
                                 1244                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0031      0.028     -0.109      0.913      -0.058       0.052
T.L1          -0.0108      0.028     -0.382      0.703      -0.066       0.045
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1          -92.3852           +0.0000j           92.3852            0.5000
-----------------------------------------------------------------------------


[T | 24h] - AR(1) model
- Train size: 311
- Trainable params: 2
- Training time (s): 0.002073049545288086
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                  311
Model:                     AutoReg(1)   Log Likelihood                -436.583
Method:               Conditional MLE   S.D. of innovations              0.989
Date:                Sun, 31 Aug 2025   AIC                            879.167
Time:                        14:45:35   BIC                            890.376
Sample:                             1   HQIC                           883.648
                                  311                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0214      0.056     -0.380      0.704      -0.132       0.089
T.L1           0.0797      0.057      1.407      0.159      -0.031       0.191
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           12.5533           +0.0000j           12.5533            0.0000
-----------------------------------------------------------------------------


[T | 1h] - AR(8) model
- Train size: 7466 | IC: bic | max_lags: 168
- Selected lags (IC): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
- 8 significant lags are kept (p<0.05): [1, 2, 3, 4, 7, 22, 27, 28] (We reuse these lags during walk-forward, we could re-search each step, but we choose speed!)
- Trainable params: 9
- Training time (s): 0.005
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 7466
Model:             Restr. AutoReg(28)   Log Likelihood                7955.239
Method:               Conditional MLE   S.D. of innovations              0.083
Date:                Sun, 31 Aug 2025   AIC                         -15890.478
Time:                        14:45:39   BIC                         -15821.334
Sample:                            28   HQIC                        -15866.725
                                 7466                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0001      0.001      0.154      0.878      -0.002       0.002
T.L1           1.5153      0.012    130.593      0.000       1.493       1.538
T.L2          -0.5882      0.021    -27.947      0.000      -0.629      -0.547
T.L3           0.0857      0.021      3.986      0.000       0.044       0.128
T.L4          -0.0638      0.014     -4.548      0.000      -0.091      -0.036
T.L7           0.0122      0.004      3.175      0.001       0.005       0.020
T.L22          0.1051      0.003     38.905      0.000       0.100       0.110
T.L27         -0.1558      0.011    -14.357      0.000      -0.177      -0.135
T.L28          0.0824      0.010      8.110      0.000       0.062       0.102
                                    Roots                                     
==============================================================================
                   Real          Imaginary           Modulus         Frequency
------------------------------------------------------------------------------
AR.1            -1.0946           -0.0000j            1.0946           -0.5000
AR.2            -1.0662           -0.2701j            1.0998           -0.4605
AR.3            -1.0662           +0.2701j            1.0998            0.4605
AR.4            -0.9835           -0.5168j            1.1110           -0.4230
AR.5            -0.9835           +0.5168j            1.1110            0.4230
AR.6            -0.8420           -0.7195j            1.1075           -0.3875
AR.7            -0.8420           +0.7195j            1.1075            0.3875
AR.8            -0.6409           -0.8811j            1.0896           -0.3501
AR.9            -0.6409           +0.8811j            1.0896            0.3501
AR.10           -0.4019           -1.0007j            1.0784           -0.3108
AR.11           -0.4019           +1.0007j            1.0784            0.3108
AR.12           -0.1448           -1.0710j            1.0807           -0.2714
AR.13           -0.1448           +1.0710j            1.0807            0.2714
AR.14            0.1088           -1.0914j            1.0968           -0.2342
AR.15            0.1088           +1.0914j            1.0968            0.2342
AR.16            0.3362           -1.0416j            1.0945           -0.2003
AR.17            0.3362           +1.0416j            1.0945            0.2003
AR.18            0.5527           -0.9089j            1.0637           -0.1631
AR.19            0.5527           +0.9089j            1.0637            0.1631
AR.20            0.7442           -0.7224j            1.0371           -0.1226
AR.21            0.7442           +0.7224j            1.0371            0.1226
AR.22            0.8864           -0.5006j            1.0179           -0.0818
AR.23            0.8864           +0.5006j            1.0179            0.0818
AR.24            0.9752           -0.2585j            1.0089           -0.0412
AR.25            0.9752           +0.2585j            1.0089            0.0412
AR.26            1.0087           -0.0000j            1.0087           -0.0000
AR.27            1.1001           -0.0000j            1.1001           -0.0000
AR.28            1.8284           -0.0000j            1.8284           -0.0000
------------------------------------------------------------------------------


[T | 6h] - AR(14) model
- Train size: 1244 | IC: bic | max_lags: 56
- Selected lags (IC): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
- 14 significant lags are kept (p<0.05): [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17] (We reuse these lags during walk-forward, we could re-search each step, but we choose speed!)
- Trainable params: 15
- Training time (s): 0.0021
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 1244
Model:             Restr. AutoReg(17)   Log Likelihood                -848.505
Method:               Conditional MLE   S.D. of innovations              0.483
Date:                Sun, 31 Aug 2025   AIC                           1729.010
Time:                        14:45:58   BIC                           1810.808
Sample:                            17   HQIC                          1759.789
                                 1244                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0071      0.014     -0.512      0.609      -0.034       0.020
T.L2          -0.4186      0.028    -14.811      0.000      -0.474      -0.363
T.L3          -0.0856      0.028     -3.096      0.002      -0.140      -0.031
T.L4           0.1689      0.030      5.588      0.000       0.110       0.228
T.L5          -0.1748      0.030     -5.909      0.000      -0.233      -0.117
T.L6          -0.1007      0.029     -3.425      0.001      -0.158      -0.043
T.L7          -0.1610      0.030     -5.298      0.000      -0.221      -0.101
T.L9          -0.1253      0.031     -4.062      0.000      -0.186      -0.065
T.L10         -0.0863      0.028     -3.073      0.002      -0.141      -0.031
T.L11         -0.1313      0.031     -4.271      0.000      -0.192      -0.071
T.L13         -0.1257      0.030     -4.181      0.000      -0.185      -0.067
T.L14         -0.0961      0.029     -3.338      0.001      -0.152      -0.040
T.L15         -0.1460      0.030     -4.921      0.000      -0.204      -0.088
T.L16          0.0822      0.028      2.954      0.003       0.028       0.137
T.L17         -0.1001      0.028     -3.610      0.000      -0.154      -0.046
                                    Roots                                     
==============================================================================
                   Real          Imaginary           Modulus         Frequency
------------------------------------------------------------------------------
AR.1            -1.0367           -0.0000j            1.0367           -0.5000
AR.2            -1.0364           -0.4448j            1.1279           -0.4355
AR.3            -1.0364           +0.4448j            1.1279            0.4355
AR.4            -0.7916           -0.8177j            1.1381           -0.3724
AR.5            -0.7916           +0.8177j            1.1381            0.3724
AR.6            -0.4091           -1.0303j            1.1086           -0.3102
AR.7            -0.4091           +1.0303j            1.1086            0.3102
AR.8            -0.0008           -1.0080j            1.0080           -0.2501
AR.9            -0.0008           +1.0080j            1.0080            0.2501
AR.10            1.0385           -0.3090j            1.0835           -0.0460
AR.11            1.0385           +0.3090j            1.0835            0.0460
AR.12            0.9199           -0.7197j            1.1679           -0.1057
AR.13            0.9199           +0.7197j            1.1679            0.1057
AR.14            0.4339           -1.1195j            1.2006           -0.1912
AR.15            0.4339           +1.1195j            1.2006            0.1912
AR.16            0.7747           -1.1950j            1.4242           -0.1585
AR.17            0.7747           +1.1950j            1.4242            0.1585
------------------------------------------------------------------------------


[T | 24h] - AR(3) model
- Train size: 311 | IC: bic | max_lags: 30
- Selected lags (IC): [1, 2, 3, 4, 5]
- 3 significant lags are kept (p<0.05): [2, 3, 5] (We reuse these lags during walk-forward, we could re-search each step, but we choose speed!)
- Trainable params: 4
- Training time (s): 0.001
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                  311
Model:              Restr. AutoReg(5)   Log Likelihood                -411.662
Method:               Conditional MLE   S.D. of innovations              0.929
Date:                Sun, 31 Aug 2025   AIC                            833.325
Time:                        14:46:01   BIC                            851.943
Sample:                             5   HQIC                           840.771
                                  311                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0432      0.053     -0.812      0.417      -0.147       0.061
T.L2          -0.2570      0.055     -4.671      0.000      -0.365      -0.149
T.L3          -0.2360      0.056     -4.220      0.000      -0.346      -0.126
T.L5          -0.1425      0.057     -2.481      0.013      -0.255      -0.030
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           -1.4255           -0.0000j            1.4255           -0.5000
AR.2            0.9187           -1.1187j            1.4476           -0.1406
AR.3            0.9187           +1.1187j            1.4476            0.1406
AR.4           -0.2059           -1.5189j            1.5328           -0.2714
AR.5           -0.2059           +1.5189j            1.5328            0.2714
-----------------------------------------------------------------------------


[T | 1h] - RNN model Univariate
- Train size: 6076 | Batch size: 32
- Training time (s): 197.7429
- Trainable params: 1153
- Best Val Loss (MSE): 0.002566
- Stopped epoch: 103
- Model's Summary:
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - RNN model Univariate
- Train size: 996 | Batch size: 32
- Training time (s): 35.0826
- Trainable params: 1153
- Best Val Loss (MSE): 0.085334
- Stopped epoch: 191
- Model's Summary:
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - RNN model Univariate
- Train size: 235 | Batch size: 32
- Training time (s): 0.4647
- Trainable params: 1153
- Best Val Loss (MSE): 0.188644
- Stopped epoch: 12
- Model's Summary:
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - RNN_MV model Multivariate
- Train size: 6076 | Batch size: 32
- Training time (s): 306.0387
- Trainable params: 1217
- Best Val Loss (MSE): 0.002291
- Stopped epoch: 123
- Model's Summary:
RNN(
  (rnn): RNN(3, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - RNN_MV model Multivariate
- Train size: 997 | Batch size: 32
- Training time (s): 16.9577
- Trainable params: 1217
- Best Val Loss (MSE): 0.067128
- Stopped epoch: 135
- Model's Summary:
RNN(
  (rnn): RNN(3, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - RNN_MV model Multivariate
- Train size: 236 | Batch size: 32
- Training time (s): 0.2918
- Trainable params: 1217
- Best Val Loss (MSE): 0.421355
- Stopped epoch: 11
- Model's Summary:
RNN(
  (rnn): RNN(3, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - ATT model Univariate
- Train size: 6076 | Batch size: 32
- Training time (s): 281.0838
- Trainable params: 10945
- Best Val Loss (MSE): 0.002934
- Stopped epoch: 111
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=1, out_features=32, bias=True)
  (layers): ModuleList(
    (0): ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - ATT model Univariate
- Train size: 996 | Batch size: 32
- Training time (s): 10.6655
- Trainable params: 9537
- Best Val Loss (MSE): 0.110156
- Stopped epoch: 38
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=1, out_features=32, bias=True)
  (layers): ModuleList(
    (0): ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - ATT model Univariate
- Train size: 235 | Batch size: 32
- Training time (s): 0.6332
- Trainable params: 9313
- Best Val Loss (MSE): 0.190522
- Stopped epoch: 12
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=1, out_features=32, bias=True)
  (layers): ModuleList(
    (0): ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - ATT_MV model Multivariate
- Train size: 6076 | Batch size: 32
- Training time (s): 142.6808
- Trainable params: 11009
- Best Val Loss (MSE): 0.002948
- Stopped epoch: 66
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0): ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - ATT_MV model Multivariate
- Train size: 997 | Batch size: 32
- Training time (s): 16.5674
- Trainable params: 9601
- Best Val Loss (MSE): 0.077664
- Stopped epoch: 61
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0): ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - ATT_MV model Multivariate
- Train size: 236 | Batch size: 32
- Training time (s): 1.3431
- Trainable params: 9377
- Best Val Loss (MSE): 0.340290
- Stopped epoch: 22
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0): ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - TRANS model Univariate
- Train size: 6076 | Batch size: 32
- Training time (s): 250.8536
- Trainable params: 8641
- Best Val Loss (MSE): 0.003219
- Stopped epoch: 91
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=1, out_features=32, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - TRANS model Univariate
- Train size: 996 | Batch size: 32
- Training time (s): 21.7515
- Trainable params: 8641
- Best Val Loss (MSE): 0.116460
- Stopped epoch: 62
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=1, out_features=32, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - TRANS model Univariate
- Train size: 235 | Batch size: 32
- Training time (s): 1.1343
- Trainable params: 8641
- Best Val Loss (MSE): 0.186974
- Stopped epoch: 14
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=1, out_features=32, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - TRANS_MV model Multivariate
- Train size: 6076 | Batch size: 32
- Training time (s): 202.7085
- Trainable params: 8705
- Best Val Loss (MSE): 0.002936
- Stopped epoch: 71
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=3, out_features=32, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - TRANS_MV model Multivariate
- Train size: 997 | Batch size: 32
- Training time (s): 29.6444
- Trainable params: 8705
- Best Val Loss (MSE): 0.097098
- Stopped epoch: 82
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=3, out_features=32, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - TRANS_MV model Multivariate
- Train size: 236 | Batch size: 32
- Training time (s): 0.8191
- Trainable params: 8705
- Best Val Loss (MSE): 0.384350
- Stopped epoch: 12
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=3, out_features=32, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
        )
        (linear1): Linear(in_features=32, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=64, out_features=32, bias=True)
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=32, out_features=1, bias=True)
)

