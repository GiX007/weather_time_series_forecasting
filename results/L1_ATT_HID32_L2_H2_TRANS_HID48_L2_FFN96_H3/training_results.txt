==== Training Results ====

[T | 1h] - AR(1) model
- Train size: 7466
- Trainable params: 2
- Training time (s): 0.0031397342681884766
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 7466
Model:                     AutoReg(1)   Log Likelihood                4340.528
Method:               Conditional MLE   S.D. of innovations              0.135
Date:                Sun, 31 Aug 2025   AIC                          -8675.056
Time:                        11:50:29   BIC                          -8654.302
Sample:                             1   HQIC                         -8667.927
                                 7466                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const      -4.057e-06      0.002     -0.003      0.998      -0.003       0.003
T.L1           0.9899      0.002    601.255      0.000       0.987       0.993
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1            1.0102           +0.0000j            1.0102            0.0000
-----------------------------------------------------------------------------


[T | 6h] - AR(1) model
- Train size: 1244
- Trainable params: 2
- Training time (s): 0.002049684524536133
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 1244
Model:                     AutoReg(1)   Log Likelihood               -1753.245
Method:               Conditional MLE   S.D. of innovations              0.992
Date:                Sun, 31 Aug 2025   AIC                           3512.489
Time:                        11:50:42   BIC                           3527.865
Sample:                             1   HQIC                          3518.271
                                 1244                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0031      0.028     -0.109      0.913      -0.058       0.052
T.L1          -0.0108      0.028     -0.382      0.703      -0.066       0.045
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1          -92.3852           +0.0000j           92.3852            0.5000
-----------------------------------------------------------------------------


[T | 24h] - AR(1) model
- Train size: 311
- Trainable params: 2
- Training time (s): 0.001657247543334961
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                  311
Model:                     AutoReg(1)   Log Likelihood                -436.583
Method:               Conditional MLE   S.D. of innovations              0.989
Date:                Sun, 31 Aug 2025   AIC                            879.167
Time:                        11:50:44   BIC                            890.376
Sample:                             1   HQIC                           883.648
                                  311                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0214      0.056     -0.380      0.704      -0.132       0.089
T.L1           0.0797      0.057      1.407      0.159      -0.031       0.191
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           12.5533           +0.0000j           12.5533            0.0000
-----------------------------------------------------------------------------


[T | 1h] - AR(8) model
- Train size: 7466 | IC: bic | max_lags: 168
- Selected lags (IC): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
- 8 significant lags are kept (p<0.05): [1, 2, 3, 4, 7, 22, 27, 28] (We reuse these lags during walk-forward, we could re-search each step, but we choose speed!)
- Trainable params: 9
- Training time (s): 0.0058
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 7466
Model:             Restr. AutoReg(28)   Log Likelihood                7955.239
Method:               Conditional MLE   S.D. of innovations              0.083
Date:                Sun, 31 Aug 2025   AIC                         -15890.478
Time:                        11:50:49   BIC                         -15821.334
Sample:                            28   HQIC                        -15866.725
                                 7466                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0001      0.001      0.154      0.878      -0.002       0.002
T.L1           1.5153      0.012    130.593      0.000       1.493       1.538
T.L2          -0.5882      0.021    -27.947      0.000      -0.629      -0.547
T.L3           0.0857      0.021      3.986      0.000       0.044       0.128
T.L4          -0.0638      0.014     -4.548      0.000      -0.091      -0.036
T.L7           0.0122      0.004      3.175      0.001       0.005       0.020
T.L22          0.1051      0.003     38.905      0.000       0.100       0.110
T.L27         -0.1558      0.011    -14.357      0.000      -0.177      -0.135
T.L28          0.0824      0.010      8.110      0.000       0.062       0.102
                                    Roots                                     
==============================================================================
                   Real          Imaginary           Modulus         Frequency
------------------------------------------------------------------------------
AR.1            -1.0946           -0.0000j            1.0946           -0.5000
AR.2            -1.0662           -0.2701j            1.0998           -0.4605
AR.3            -1.0662           +0.2701j            1.0998            0.4605
AR.4            -0.9835           -0.5168j            1.1110           -0.4230
AR.5            -0.9835           +0.5168j            1.1110            0.4230
AR.6            -0.8420           -0.7195j            1.1075           -0.3875
AR.7            -0.8420           +0.7195j            1.1075            0.3875
AR.8            -0.6409           -0.8811j            1.0896           -0.3501
AR.9            -0.6409           +0.8811j            1.0896            0.3501
AR.10           -0.4019           -1.0007j            1.0784           -0.3108
AR.11           -0.4019           +1.0007j            1.0784            0.3108
AR.12           -0.1448           -1.0710j            1.0807           -0.2714
AR.13           -0.1448           +1.0710j            1.0807            0.2714
AR.14            0.1088           -1.0914j            1.0968           -0.2342
AR.15            0.1088           +1.0914j            1.0968            0.2342
AR.16            0.3362           -1.0416j            1.0945           -0.2003
AR.17            0.3362           +1.0416j            1.0945            0.2003
AR.18            0.5527           -0.9089j            1.0637           -0.1631
AR.19            0.5527           +0.9089j            1.0637            0.1631
AR.20            0.7442           -0.7224j            1.0371           -0.1226
AR.21            0.7442           +0.7224j            1.0371            0.1226
AR.22            0.8864           -0.5006j            1.0179           -0.0818
AR.23            0.8864           +0.5006j            1.0179            0.0818
AR.24            0.9752           -0.2585j            1.0089           -0.0412
AR.25            0.9752           +0.2585j            1.0089            0.0412
AR.26            1.0087           -0.0000j            1.0087           -0.0000
AR.27            1.1001           -0.0000j            1.1001           -0.0000
AR.28            1.8284           -0.0000j            1.8284           -0.0000
------------------------------------------------------------------------------


[T | 6h] - AR(14) model
- Train size: 1244 | IC: bic | max_lags: 56
- Selected lags (IC): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
- 14 significant lags are kept (p<0.05): [2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17] (We reuse these lags during walk-forward, we could re-search each step, but we choose speed!)
- Trainable params: 15
- Training time (s): 0.002
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                 1244
Model:             Restr. AutoReg(17)   Log Likelihood                -848.505
Method:               Conditional MLE   S.D. of innovations              0.483
Date:                Sun, 31 Aug 2025   AIC                           1729.010
Time:                        11:51:10   BIC                           1810.808
Sample:                            17   HQIC                          1759.789
                                 1244                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0071      0.014     -0.512      0.609      -0.034       0.020
T.L2          -0.4186      0.028    -14.811      0.000      -0.474      -0.363
T.L3          -0.0856      0.028     -3.096      0.002      -0.140      -0.031
T.L4           0.1689      0.030      5.588      0.000       0.110       0.228
T.L5          -0.1748      0.030     -5.909      0.000      -0.233      -0.117
T.L6          -0.1007      0.029     -3.425      0.001      -0.158      -0.043
T.L7          -0.1610      0.030     -5.298      0.000      -0.221      -0.101
T.L9          -0.1253      0.031     -4.062      0.000      -0.186      -0.065
T.L10         -0.0863      0.028     -3.073      0.002      -0.141      -0.031
T.L11         -0.1313      0.031     -4.271      0.000      -0.192      -0.071
T.L13         -0.1257      0.030     -4.181      0.000      -0.185      -0.067
T.L14         -0.0961      0.029     -3.338      0.001      -0.152      -0.040
T.L15         -0.1460      0.030     -4.921      0.000      -0.204      -0.088
T.L16          0.0822      0.028      2.954      0.003       0.028       0.137
T.L17         -0.1001      0.028     -3.610      0.000      -0.154      -0.046
                                    Roots                                     
==============================================================================
                   Real          Imaginary           Modulus         Frequency
------------------------------------------------------------------------------
AR.1            -1.0367           -0.0000j            1.0367           -0.5000
AR.2            -1.0364           -0.4448j            1.1279           -0.4355
AR.3            -1.0364           +0.4448j            1.1279            0.4355
AR.4            -0.7916           -0.8177j            1.1381           -0.3724
AR.5            -0.7916           +0.8177j            1.1381            0.3724
AR.6            -0.4091           -1.0303j            1.1086           -0.3102
AR.7            -0.4091           +1.0303j            1.1086            0.3102
AR.8            -0.0008           -1.0080j            1.0080           -0.2501
AR.9            -0.0008           +1.0080j            1.0080            0.2501
AR.10            1.0385           -0.3090j            1.0835           -0.0460
AR.11            1.0385           +0.3090j            1.0835            0.0460
AR.12            0.9199           -0.7197j            1.1679           -0.1057
AR.13            0.9199           +0.7197j            1.1679            0.1057
AR.14            0.4339           -1.1195j            1.2006           -0.1912
AR.15            0.4339           +1.1195j            1.2006            0.1912
AR.16            0.7747           -1.1950j            1.4242           -0.1585
AR.17            0.7747           +1.1950j            1.4242            0.1585
------------------------------------------------------------------------------


[T | 24h] - AR(3) model
- Train size: 311 | IC: bic | max_lags: 30
- Selected lags (IC): [1, 2, 3, 4, 5]
- 3 significant lags are kept (p<0.05): [2, 3, 5] (We reuse these lags during walk-forward, we could re-search each step, but we choose speed!)
- Trainable params: 4
- Training time (s): 0.002
- Model's Summary:
                            AutoReg Model Results                             
==============================================================================
Dep. Variable:                      T   No. Observations:                  311
Model:              Restr. AutoReg(5)   Log Likelihood                -411.662
Method:               Conditional MLE   S.D. of innovations              0.929
Date:                Sun, 31 Aug 2025   AIC                            833.325
Time:                        11:51:12   BIC                            851.943
Sample:                             5   HQIC                           840.771
                                  311                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0432      0.053     -0.812      0.417      -0.147       0.061
T.L2          -0.2570      0.055     -4.671      0.000      -0.365      -0.149
T.L3          -0.2360      0.056     -4.220      0.000      -0.346      -0.126
T.L5          -0.1425      0.057     -2.481      0.013      -0.255      -0.030
                                    Roots                                    
=============================================================================
                  Real          Imaginary           Modulus         Frequency
-----------------------------------------------------------------------------
AR.1           -1.4255           -0.0000j            1.4255           -0.5000
AR.2            0.9187           -1.1187j            1.4476           -0.1406
AR.3            0.9187           +1.1187j            1.4476            0.1406
AR.4           -0.2059           -1.5189j            1.5328           -0.2714
AR.5           -0.2059           +1.5189j            1.5328            0.2714
-----------------------------------------------------------------------------


[T | 1h] - RNN model Univariate
- Train size: 6028 | Batch size: 32
- Training time (s): 225.6078
- Trainable params: 1153
- Best Val Loss (MSE): 0.002522
- Stopped epoch: 73
- Model's Summary:
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - RNN model Univariate
- Train size: 968 | Batch size: 32
- Training time (s): 5.0174
- Trainable params: 1153
- Best Val Loss (MSE): 0.105414
- Stopped epoch: 18
- Model's Summary:
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - RNN model Univariate
- Train size: 228 | Batch size: 32
- Training time (s): 0.5512
- Trainable params: 1153
- Best Val Loss (MSE): 0.378264
- Stopped epoch: 13
- Model's Summary:
RNN(
  (rnn): RNN(1, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - RNN_MV model Multivariate
- Train size: 6028 | Batch size: 32
- Training time (s): 238.569
- Trainable params: 1217
- Best Val Loss (MSE): 0.002189
- Stopped epoch: 75
- Model's Summary:
RNN(
  (rnn): RNN(3, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - RNN_MV model Multivariate
- Train size: 969 | Batch size: 32
- Training time (s): 29.1706
- Trainable params: 1217
- Best Val Loss (MSE): 0.077022
- Stopped epoch: 105
- Model's Summary:
RNN(
  (rnn): RNN(3, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - RNN_MV model Multivariate
- Train size: 229 | Batch size: 32
- Training time (s): 0.3411
- Trainable params: 1217
- Best Val Loss (MSE): 0.393910
- Stopped epoch: 11
- Model's Summary:
RNN(
  (rnn): RNN(3, 32, batch_first=True)
  (fc): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - ATT model Univariate
- Train size: 6028 | Batch size: 32
- Training time (s): 667.3014
- Trainable params: 21025
- Best Val Loss (MSE): 0.003410
- Stopped epoch: 109
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=1, out_features=32, bias=True)
  (layers): ModuleList(
    (0-1): 2 x ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - ATT model Univariate
- Train size: 968 | Batch size: 32
- Training time (s): 14.5952
- Trainable params: 18977
- Best Val Loss (MSE): 0.135898
- Stopped epoch: 24
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=1, out_features=32, bias=True)
  (layers): ModuleList(
    (0-1): 2 x ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - ATT model Univariate
- Train size: 228 | Batch size: 32
- Training time (s): 3.755
- Trainable params: 18081
- Best Val Loss (MSE): 0.376845
- Stopped epoch: 21
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=1, out_features=32, bias=True)
  (layers): ModuleList(
    (0-1): 2 x ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - ATT_MV model Multivariate
- Train size: 6028 | Batch size: 32
- Training time (s): 246.8591
- Trainable params: 21089
- Best Val Loss (MSE): 0.003638
- Stopped epoch: 43
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0-1): 2 x ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 6h] - ATT_MV model Multivariate
- Train size: 969 | Batch size: 32
- Training time (s): 54.27
- Trainable params: 19041
- Best Val Loss (MSE): 0.089241
- Stopped epoch: 92
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0-1): 2 x ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 24h] - ATT_MV model Multivariate
- Train size: 229 | Batch size: 32
- Training time (s): 2.5273
- Trainable params: 18145
- Best Val Loss (MSE): 0.329648
- Stopped epoch: 28
- Model's Summary:
AttNN(
  (in_proj): Linear(in_features=3, out_features=32, bias=True)
  (layers): ModuleList(
    (0-1): 2 x ModuleDict(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
      )
      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=32, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=32, bias=True)
      )
      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
    )
  )
  (out): Linear(in_features=32, out_features=1, bias=True)
)

[T | 1h] - TRANS model Univariate
- Train size: 6028 | Batch size: 32
- Training time (s): 933.6536
- Trainable params: 38065
- Best Val Loss (MSE): 0.003020
- Stopped epoch: 86
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=1, out_features=48, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=96, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=96, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=48, out_features=1, bias=True)
)

[T | 6h] - TRANS model Univariate
- Train size: 968 | Batch size: 32
- Training time (s): 19.5148
- Trainable params: 38065
- Best Val Loss (MSE): 0.141621
- Stopped epoch: 30
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=1, out_features=48, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=96, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=96, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=48, out_features=1, bias=True)
)

[T | 24h] - TRANS model Univariate
- Train size: 228 | Batch size: 32
- Training time (s): 1.4925
- Trainable params: 38065
- Best Val Loss (MSE): 0.358113
- Stopped epoch: 13
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=1, out_features=48, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=96, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=96, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=48, out_features=1, bias=True)
)

[T | 1h] - TRANS_MV model Multivariate
- Train size: 6028 | Batch size: 32
- Training time (s): 585.6651
- Trainable params: 38161
- Best Val Loss (MSE): 0.003192
- Stopped epoch: 56
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=3, out_features=48, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=96, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=96, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=48, out_features=1, bias=True)
)

[T | 6h] - TRANS_MV model Multivariate
- Train size: 969 | Batch size: 32
- Training time (s): 42.2326
- Trainable params: 38161
- Best Val Loss (MSE): 0.092438
- Stopped epoch: 65
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=3, out_features=48, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=96, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=96, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=48, out_features=1, bias=True)
)

[T | 24h] - TRANS_MV model Multivariate
- Train size: 229 | Batch size: 32
- Training time (s): 2.4473
- Trainable params: 38161
- Best Val Loss (MSE): 0.354126
- Stopped epoch: 20
- Model's Summary:
TransformerEncoder(
  (input_proj): Linear(in_features=3, out_features=48, bias=True)
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=96, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=96, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=48, out_features=1, bias=True)
)

